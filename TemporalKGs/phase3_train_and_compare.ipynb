{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/hasanmansoor96/RCS\n",
        "\n",
        "# List the contents of the cloned repository to confirm\n",
        "repo_name = \"RCS\"\n",
        "if os.path.exists(repo_name):\n",
        "    print(f\"\\nContents of '{repo_name}':\")\n",
        "    !ls -F {repo_name}\n",
        "else:\n",
        "    print(f\"Error: Repository '{repo_name}' not found after cloning.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnWn8DmnDdor",
        "outputId": "2e6b6b66-8ac1-4e38-a399-6f84a8755bb1"
      },
      "id": "EnWn8DmnDdor",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RCS'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 60 (delta 25), reused 26 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (60/60), 2.35 MiB | 8.82 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n",
            "Filtering content: 100% (5/5), 246.72 MiB | 22.96 MiB/s, done.\n",
            "\n",
            "Contents of 'RCS':\n",
            "README.md  TemporalKGs/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o25i10iNDVrI"
      },
      "source": [
        "# Phase 3: Baseline vs Influence-Aware Temporal Prediction\n",
        "\n",
        "\"\n",
        "\"This notebook answers the core question:\n",
        "\"\n",
        "\"**Does using `final_influence_graph.json` improve temporal link prediction?**\n",
        "\n",
        "\"\n",
        "\"It runs two conditions on ICEWS normalized files:\n",
        "\"\n",
        "\"1. Baseline temporal model (`TTransE-lite`)\n",
        "\"\n",
        "\"2. Baseline + influence-aware auxiliary updates\n",
        "\n",
        "\"\n",
        "\"Metrics: `Hits@3`, `Hits@5`, `Hits@10`, `MRR`\n",
        "\"\n"
      ],
      "id": "o25i10iNDVrI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRRaGe5nDVrK"
      },
      "source": [
        "## 1) Why Time-Based Split Matters\n",
        "\n",
        "\"\n",
        "\"If train/valid/test are random across all years, the model can train on future events and then be evaluated on earlier events.\n",
        "\"\n",
        "\"That leaks temporal information and can inflate ranking metrics.\n",
        "\n",
        "\"\n",
        "\"For the question *\"predict at time `t+Î”`\"*, evaluation must respect chronology:\n",
        "\"\n",
        "\"- Train on earlier time windows\n",
        "\"\n",
        "\"- Tune on later windows\n",
        "\"\n",
        "\"- Test on the latest windows\n",
        "\n",
        "\"\n",
        "\"This notebook still supports your current split files, but the evaluation target is truly faithful only with chronological splitting.\n",
        "\"\n"
      ],
      "id": "eRRaGe5nDVrK"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz7OqwtrDVrK",
        "outputId": "6dc7d68c-78b6-4629-edaf-e8fc5cc09971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import DefaultDict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def get_or_add(mapping: dict[str, int], key: str) -> int:\n",
        "    value = mapping.get(key)\n",
        "    if value is None:\n",
        "        value = len(mapping)\n",
        "        mapping[key] = value\n",
        "    return value\n",
        "\n",
        "\n",
        "set_seed(7)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "id": "Uz7OqwtrDVrK"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Cx7Gjf-ODVrL"
      },
      "outputs": [],
      "source": [
        "# Config\n",
        "\n",
        "SCRIPT_DIR = Path(\"/content/RCS/TemporalKGs\")\n",
        "DATA_DIR = SCRIPT_DIR / \"/content/RCS/TemporalKGs/icews05-15_aug_inverse_time_year\"\n",
        "INFLUENCE_GRAPH_PATH = SCRIPT_DIR / \"/content/RCS/TemporalKGs/final_influence_graph.json\"\n",
        "OUTPUT_DIR = SCRIPT_DIR / \"/content/RCS/TemporalKGs\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RUN_MODE = \"both\"  # baseline | influence | both\n",
        "\n",
        "EMB_DIM = 128\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 2048\n",
        "EVAL_BATCH_SIZE = 32\n",
        "EVAL_MAX_SAMPLES = 20000  # <=0 for full split\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-6\n",
        "GRAD_CLIP = 5.0\n",
        "LOG_EVERY_STEPS = 100\n",
        "\n",
        "INFLUENCE_LAMBDA = 0.05\n",
        "MAX_INFLUENCE_NEIGHBORS = 5\n"
      ],
      "id": "Cx7Gjf-ODVrL"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "iQpdYMK3DVrM"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TripleSplit:\n",
        "    h: torch.Tensor\n",
        "    r: torch.Tensor\n",
        "    t: torch.Tensor\n",
        "    time_id: torch.Tensor\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return int(self.h.shape[0])\n",
        "\n",
        "\n",
        "class TemporalTransELite(nn.Module):\n",
        "    def __init__(self, num_entities: int, num_relations: int, num_times: int, emb_dim: int) -> None:\n",
        "        super().__init__()\n",
        "        self.entity_emb = nn.Embedding(num_entities, emb_dim)\n",
        "        self.relation_emb = nn.Embedding(num_relations, emb_dim)\n",
        "        self.time_emb = nn.Embedding(num_times, emb_dim)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        nn.init.xavier_uniform_(self.entity_emb.weight)\n",
        "        nn.init.xavier_uniform_(self.relation_emb.weight)\n",
        "        nn.init.xavier_uniform_(self.time_emb.weight)\n",
        "\n",
        "    def score_triples(\n",
        "        self,\n",
        "        h_idx: torch.Tensor,\n",
        "        r_idx: torch.Tensor,\n",
        "        t_idx: torch.Tensor,\n",
        "        time_idx: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        h = self.entity_emb(h_idx)\n",
        "        r = self.relation_emb(r_idx)\n",
        "        tail = self.entity_emb(t_idx)\n",
        "        time_vec = self.time_emb(time_idx)\n",
        "        return -(h + r + time_vec - tail).abs().sum(dim=1)\n",
        "\n",
        "    def score_all_tails(\n",
        "        self,\n",
        "        h_idx: torch.Tensor,\n",
        "        r_idx: torch.Tensor,\n",
        "        time_idx: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        query = self.entity_emb(h_idx) + self.relation_emb(r_idx) + self.time_emb(time_idx)\n",
        "        return -torch.cdist(query, self.entity_emb.weight, p=1.0)\n"
      ],
      "id": "iQpdYMK3DVrM"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0Ua1Ci3mDVrM"
      },
      "outputs": [],
      "source": [
        "def encode_split(\n",
        "    path: Path,\n",
        "    entity_to_id: dict[str, int],\n",
        "    relation_to_id: dict[str, int],\n",
        "    time_to_id: dict[int, int],\n",
        ") -> TripleSplit:\n",
        "    h_idx: list[int] = []\n",
        "    r_idx: list[int] = []\n",
        "    t_idx: list[int] = []\n",
        "    time_idx: list[int] = []\n",
        "\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f, delimiter=\"\t\")\n",
        "        for row in reader:\n",
        "            h_idx.append(get_or_add(entity_to_id, row[\"head\"]))\n",
        "            r_idx.append(get_or_add(relation_to_id, row[\"relation\"]))\n",
        "            t_idx.append(get_or_add(entity_to_id, row[\"tail\"]))\n",
        "            time_raw = int(row[\"time_index\"])\n",
        "            time_idx.append(get_or_add(time_to_id, time_raw))\n",
        "\n",
        "    return TripleSplit(\n",
        "        h=torch.tensor(h_idx, dtype=torch.long),\n",
        "        r=torch.tensor(r_idx, dtype=torch.long),\n",
        "        t=torch.tensor(t_idx, dtype=torch.long),\n",
        "        time_id=torch.tensor(time_idx, dtype=torch.long),\n",
        "    )\n",
        "\n",
        "\n",
        "def load_data(data_dir: Path) -> tuple[TripleSplit, TripleSplit, TripleSplit, dict[str, int], dict[str, int], dict[int, int]]:\n",
        "    train_path = data_dir / \"/content/RCS/TemporalKGs/icews05-15_aug_inverse_time_year/icews_2005-2015_train_normalized.txt\"\n",
        "    valid_path = data_dir / \"/content/RCS/TemporalKGs/icews05-15_aug_inverse_time_year/icews_2005-2015_valid_normalized.txt\"\n",
        "    test_path = data_dir / \"/content/RCS/TemporalKGs/icews05-15_aug_inverse_time_year/icews_2005-2015_test_normalized.txt\"\n",
        "\n",
        "    for p in (train_path, valid_path, test_path):\n",
        "        if not p.exists():\n",
        "            raise FileNotFoundError(f\"Missing data file: {p}\")\n",
        "\n",
        "    entity_to_id: dict[str, int] = {}\n",
        "    relation_to_id: dict[str, int] = {}\n",
        "    time_to_id: dict[int, int] = {}\n",
        "\n",
        "    train = encode_split(train_path, entity_to_id, relation_to_id, time_to_id)\n",
        "    valid = encode_split(valid_path, entity_to_id, relation_to_id, time_to_id)\n",
        "    test = encode_split(test_path, entity_to_id, relation_to_id, time_to_id)\n",
        "\n",
        "    print(\n",
        "        \"Loaded splits: \"\n",
        "        f\"train={len(train):,}, valid={len(valid):,}, test={len(test):,}, \"\n",
        "        f\"entities={len(entity_to_id):,}, relations={len(relation_to_id):,}, times={len(time_to_id):,}\"\n",
        "    )\n",
        "    return train, valid, test, entity_to_id, relation_to_id, time_to_id\n",
        "\n",
        "\n",
        "def build_true_tails(*splits: TripleSplit) -> DefaultDict[tuple[int, int, int], set[int]]:\n",
        "    true_tails: DefaultDict[tuple[int, int, int], set[int]] = defaultdict(set)\n",
        "    for split in splits:\n",
        "        h_np = split.h.numpy()\n",
        "        r_np = split.r.numpy()\n",
        "        t_np = split.t.numpy()\n",
        "        time_np = split.time_id.numpy()\n",
        "        for h, r, tail, ti in zip(h_np, r_np, t_np, time_np, strict=True):\n",
        "            true_tails[(int(h), int(r), int(ti))].add(int(tail))\n",
        "    return true_tails\n"
      ],
      "id": "0Ua1Ci3mDVrM"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1sKcNIa6DVrM"
      },
      "outputs": [],
      "source": [
        "def load_influence_adjacency(\n",
        "    influence_graph_path: Path,\n",
        "    entity_to_id: dict[str, int],\n",
        "    max_neighbors: int,\n",
        ") -> dict[int, list[tuple[int, float]]]:\n",
        "    if not influence_graph_path.exists():\n",
        "        raise FileNotFoundError(f\"Influence graph not found: {influence_graph_path}\")\n",
        "\n",
        "    with influence_graph_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        raw_graph = json.load(f)\n",
        "\n",
        "    adjacency: dict[int, list[tuple[int, float]]] = {}\n",
        "    total_edges = 0\n",
        "\n",
        "    for src_name, raw_neighbors in raw_graph.items():\n",
        "        src_id = entity_to_id.get(src_name)\n",
        "        if src_id is None:\n",
        "            continue\n",
        "\n",
        "        neighbors_sorted = sorted(raw_neighbors.items(), key=lambda kv: kv[1], reverse=True)\n",
        "        kept_neighbors: list[tuple[int, float]] = []\n",
        "\n",
        "        for dst_name, weight in neighbors_sorted:\n",
        "            dst_id = entity_to_id.get(dst_name)\n",
        "            if dst_id is None or dst_id == src_id:\n",
        "                continue\n",
        "            w = float(weight)\n",
        "            if w <= 0.0:\n",
        "                continue\n",
        "            kept_neighbors.append((dst_id, w))\n",
        "            if max_neighbors > 0 and len(kept_neighbors) >= max_neighbors:\n",
        "                break\n",
        "\n",
        "        if kept_neighbors:\n",
        "            adjacency[src_id] = kept_neighbors\n",
        "            total_edges += len(kept_neighbors)\n",
        "\n",
        "    print(f\"Influence adjacency: sources={len(adjacency):,}, edges={total_edges:,}\")\n",
        "    return adjacency\n",
        "\n",
        "\n",
        "def compute_influence_loss(\n",
        "    model: TemporalTransELite,\n",
        "    batch_entities_cpu: torch.Tensor,\n",
        "    influence_adjacency: dict[int, list[tuple[int, float]]],\n",
        "    device: torch.device,\n",
        ") -> torch.Tensor:\n",
        "    src_ids: list[int] = []\n",
        "    dst_ids: list[int] = []\n",
        "    weights: list[float] = []\n",
        "\n",
        "    for src_id in batch_entities_cpu.tolist():\n",
        "        neighbors = influence_adjacency.get(int(src_id))\n",
        "        if not neighbors:\n",
        "            continue\n",
        "        for dst_id, w in neighbors:\n",
        "            src_ids.append(int(src_id))\n",
        "            dst_ids.append(int(dst_id))\n",
        "            weights.append(float(w))\n",
        "\n",
        "    if not src_ids:\n",
        "        return torch.zeros((), device=device)\n",
        "\n",
        "    src_tensor = torch.tensor(src_ids, dtype=torch.long, device=device)\n",
        "    dst_tensor = torch.tensor(dst_ids, dtype=torch.long, device=device)\n",
        "    w_tensor = torch.tensor(weights, dtype=torch.float32, device=device)\n",
        "\n",
        "    diff = model.entity_emb(src_tensor) - model.entity_emb(dst_tensor)\n",
        "    return (w_tensor * diff.pow(2).sum(dim=1)).mean()\n"
      ],
      "id": "1sKcNIa6DVrM"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NOwfv2vyDVrM"
      },
      "outputs": [],
      "source": [
        "def train_one_condition(\n",
        "    model: TemporalTransELite,\n",
        "    train_data: TripleSplit,\n",
        "    num_entities: int,\n",
        "    influence_adjacency: dict[int, list[tuple[int, float]]] | None = None,\n",
        "    influence_lambda: float = 0.0,\n",
        ") -> None:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    num_train = len(train_data)\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        epoch_start = time.time()\n",
        "        permutation = torch.randperm(num_train)\n",
        "        epoch_loss = 0.0\n",
        "        batches = 0\n",
        "\n",
        "        model.train()\n",
        "        for start in range(0, num_train, BATCH_SIZE):\n",
        "            end = min(start + BATCH_SIZE, num_train)\n",
        "            idx = permutation[start:end]\n",
        "\n",
        "            h_cpu = train_data.h[idx]\n",
        "            r_cpu = train_data.r[idx]\n",
        "            t_cpu = train_data.t[idx]\n",
        "            time_cpu = train_data.time_id[idx]\n",
        "            neg_t_cpu = torch.randint(0, num_entities, (len(idx),), dtype=torch.long)\n",
        "\n",
        "            h = h_cpu.to(device)\n",
        "            r = r_cpu.to(device)\n",
        "            t = t_cpu.to(device)\n",
        "            time_idx = time_cpu.to(device)\n",
        "            neg_t = neg_t_cpu.to(device)\n",
        "\n",
        "            pos_scores = model.score_triples(h, r, t, time_idx)\n",
        "            neg_scores = model.score_triples(h, r, neg_t, time_idx)\n",
        "            base_loss = -F.logsigmoid(pos_scores).mean() - F.logsigmoid(-neg_scores).mean()\n",
        "\n",
        "            influence_loss = torch.zeros((), device=device)\n",
        "            if influence_adjacency and influence_lambda > 0.0:\n",
        "                batch_entities = torch.unique(torch.cat([h_cpu, t_cpu], dim=0))\n",
        "                influence_loss = compute_influence_loss(model, batch_entities, influence_adjacency, device)\n",
        "\n",
        "            loss = base_loss + influence_lambda * influence_loss\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            if GRAD_CLIP > 0:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                model.entity_emb.weight.data = F.normalize(model.entity_emb.weight.data, p=2, dim=1)\n",
        "                model.relation_emb.weight.data = F.normalize(model.relation_emb.weight.data, p=2, dim=1)\n",
        "                model.time_emb.weight.data = F.normalize(model.time_emb.weight.data, p=2, dim=1)\n",
        "\n",
        "            step += 1\n",
        "            batches += 1\n",
        "            epoch_loss += float(loss.detach().cpu())\n",
        "\n",
        "            if LOG_EVERY_STEPS > 0 and step % LOG_EVERY_STEPS == 0:\n",
        "                print(f\"step={step:,} epoch={epoch}/{EPOCHS} loss={float(loss):.4f}\")\n",
        "\n",
        "        print(\n",
        "            f\"epoch={epoch}/{EPOCHS} avg_loss={epoch_loss / max(1, batches):.4f} \"\n",
        "            f\"time={time.time() - epoch_start:.1f}s\"\n",
        "        )\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_hits_at_k(\n",
        "    model: TemporalTransELite,\n",
        "    split: TripleSplit,\n",
        "    true_tails: DefaultDict[tuple[int, int, int], set[int]],\n",
        ") -> dict[str, float]:\n",
        "    model.eval()\n",
        "    n_total = len(split)\n",
        "    n_eval = min(n_total, EVAL_MAX_SAMPLES) if EVAL_MAX_SAMPLES > 0 else n_total\n",
        "    indices = torch.arange(n_eval, dtype=torch.long)\n",
        "\n",
        "    hits3 = 0\n",
        "    hits5 = 0\n",
        "    hits10 = 0\n",
        "    mrr_sum = 0.0\n",
        "\n",
        "    for start in range(0, n_eval, EVAL_BATCH_SIZE):\n",
        "        end = min(start + EVAL_BATCH_SIZE, n_eval)\n",
        "        idx = indices[start:end]\n",
        "\n",
        "        h_cpu = split.h[idx]\n",
        "        r_cpu = split.r[idx]\n",
        "        t_cpu = split.t[idx]\n",
        "        time_cpu = split.time_id[idx]\n",
        "\n",
        "        h = h_cpu.to(device)\n",
        "        r = r_cpu.to(device)\n",
        "        t = t_cpu.to(device)\n",
        "        ti = time_cpu.to(device)\n",
        "\n",
        "        scores = model.score_all_tails(h, r, ti)\n",
        "        bsz = int(scores.shape[0])\n",
        "\n",
        "        for i in range(bsz):\n",
        "            key = (int(h_cpu[i]), int(r_cpu[i]), int(time_cpu[i]))\n",
        "            true_tail = int(t_cpu[i])\n",
        "            for other_true in true_tails[key]:\n",
        "                if other_true != true_tail:\n",
        "                    scores[i, other_true] = -1e9\n",
        "\n",
        "        true_scores = scores[torch.arange(bsz, device=device), t]\n",
        "        ranks = 1 + torch.sum(scores > true_scores.unsqueeze(1), dim=1)\n",
        "        ranks_cpu = ranks.detach().cpu()\n",
        "\n",
        "        hits3 += int((ranks_cpu <= 3).sum().item())\n",
        "        hits5 += int((ranks_cpu <= 5).sum().item())\n",
        "        hits10 += int((ranks_cpu <= 10).sum().item())\n",
        "        mrr_sum += float((1.0 / ranks_cpu.float()).sum().item())\n",
        "\n",
        "    denom = float(n_eval)\n",
        "    return {\n",
        "        \"num_eval_samples\": int(n_eval),\n",
        "        \"hits@3\": hits3 / denom,\n",
        "        \"hits@5\": hits5 / denom,\n",
        "        \"hits@10\": hits10 / denom,\n",
        "        \"mrr\": mrr_sum / denom,\n",
        "    }\n",
        "\n",
        "\n",
        "def run_condition(\n",
        "    name: str,\n",
        "    train_data: TripleSplit,\n",
        "    valid_data: TripleSplit,\n",
        "    test_data: TripleSplit,\n",
        "    true_tails: DefaultDict[tuple[int, int, int], set[int]],\n",
        "    num_entities: int,\n",
        "    num_relations: int,\n",
        "    num_times: int,\n",
        "    influence_adjacency: dict[int, list[tuple[int, float]]] | None,\n",
        "    influence_lambda: float,\n",
        ") -> dict[str, object]:\n",
        "    print(f\"\\n=== Running {name} ===\")\n",
        "    model = TemporalTransELite(num_entities, num_relations, num_times, EMB_DIM).to(device)\n",
        "\n",
        "    train_one_condition(\n",
        "        model=model,\n",
        "        train_data=train_data,\n",
        "        num_entities=num_entities,\n",
        "        influence_adjacency=influence_adjacency,\n",
        "        influence_lambda=influence_lambda,\n",
        "    )\n",
        "\n",
        "    print(f\"Evaluating {name} on valid...\")\n",
        "    valid_metrics = evaluate_hits_at_k(model, valid_data, true_tails)\n",
        "    print(valid_metrics)\n",
        "\n",
        "    print(f\"Evaluating {name} on test...\")\n",
        "    test_metrics = evaluate_hits_at_k(model, test_data, true_tails)\n",
        "    print(test_metrics)\n",
        "\n",
        "    return {\n",
        "        \"condition\": name,\n",
        "        \"influence_lambda\": influence_lambda,\n",
        "        \"valid\": valid_metrics,\n",
        "        \"test\": test_metrics,\n",
        "    }\n"
      ],
      "id": "NOwfv2vyDVrM"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjMOg8C6DVrN",
        "outputId": "5d4ea865-9fd8-44cf-9ca1-3cd06fd3907b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded splits: train=1,106,886, valid=138,825, test=138,276, entities=10,488, relations=2,775, times=132\n",
            "\n",
            "=== Running baseline ===\n",
            "step=100 epoch=1/5 loss=17.0648\n",
            "step=200 epoch=1/5 loss=15.7159\n",
            "step=300 epoch=1/5 loss=13.9225\n",
            "step=400 epoch=1/5 loss=11.6179\n",
            "step=500 epoch=1/5 loss=8.8352\n",
            "epoch=1/5 avg_loss=13.8943 time=17.6s\n",
            "step=600 epoch=2/5 loss=6.1950\n",
            "step=700 epoch=2/5 loss=4.2671\n",
            "step=800 epoch=2/5 loss=2.9889\n",
            "step=900 epoch=2/5 loss=2.4585\n",
            "step=1,000 epoch=2/5 loss=2.1411\n",
            "epoch=2/5 avg_loss=3.5414 time=16.4s\n",
            "step=1,100 epoch=3/5 loss=1.7280\n",
            "step=1,200 epoch=3/5 loss=1.5590\n",
            "step=1,300 epoch=3/5 loss=1.4753\n",
            "step=1,400 epoch=3/5 loss=1.3893\n",
            "step=1,500 epoch=3/5 loss=1.3052\n",
            "step=1,600 epoch=3/5 loss=1.2370\n",
            "epoch=3/5 avg_loss=1.4485 time=16.8s\n",
            "step=1,700 epoch=4/5 loss=1.1994\n",
            "step=1,800 epoch=4/5 loss=1.1722\n",
            "step=1,900 epoch=4/5 loss=1.1051\n",
            "step=2,000 epoch=4/5 loss=1.1179\n",
            "step=2,100 epoch=4/5 loss=1.0787\n",
            "epoch=4/5 avg_loss=1.1542 time=17.3s\n",
            "step=2,200 epoch=5/5 loss=1.0743\n",
            "step=2,300 epoch=5/5 loss=1.0864\n",
            "step=2,400 epoch=5/5 loss=1.0705\n",
            "step=2,500 epoch=5/5 loss=1.0733\n",
            "step=2,600 epoch=5/5 loss=1.0825\n",
            "step=2,700 epoch=5/5 loss=1.0674\n",
            "epoch=5/5 avg_loss=1.0798 time=17.0s\n",
            "Evaluating baseline on valid...\n",
            "{'num_eval_samples': 20000, 'hits@3': 0.20245, 'hits@5': 0.2944, 'hits@10': 0.40905, 'mrr': 0.14082264953255652}\n",
            "Evaluating baseline on test...\n",
            "{'num_eval_samples': 20000, 'hits@3': 0.20365, 'hits@5': 0.29345, 'hits@10': 0.41165, 'mrr': 0.14128677959442137}\n",
            "Influence adjacency: sources=5,395, edges=26,969\n",
            "\n",
            "=== Running influence_aware ===\n",
            "step=100 epoch=1/5 loss=17.2122\n",
            "step=200 epoch=1/5 loss=15.8716\n",
            "step=300 epoch=1/5 loss=14.0981\n",
            "step=400 epoch=1/5 loss=11.8725\n",
            "step=500 epoch=1/5 loss=9.1771\n",
            "epoch=1/5 avg_loss=14.0877 time=23.3s\n",
            "step=600 epoch=2/5 loss=6.4056\n",
            "step=700 epoch=2/5 loss=4.3472\n",
            "step=800 epoch=2/5 loss=2.9716\n",
            "step=900 epoch=2/5 loss=2.3764\n",
            "step=1,000 epoch=2/5 loss=2.1910\n",
            "epoch=2/5 avg_loss=3.6518 time=23.4s\n",
            "step=1,100 epoch=3/5 loss=1.7423\n",
            "step=1,200 epoch=3/5 loss=1.5235\n",
            "step=1,300 epoch=3/5 loss=1.4334\n",
            "step=1,400 epoch=3/5 loss=1.4276\n",
            "step=1,500 epoch=3/5 loss=1.2890\n",
            "step=1,600 epoch=3/5 loss=1.2176\n",
            "epoch=3/5 avg_loss=1.4431 time=23.6s\n",
            "step=1,700 epoch=4/5 loss=1.1738\n",
            "step=1,800 epoch=4/5 loss=1.1434\n",
            "step=1,900 epoch=4/5 loss=1.2083\n",
            "step=2,000 epoch=4/5 loss=1.1329\n",
            "step=2,100 epoch=4/5 loss=1.1096\n",
            "epoch=4/5 avg_loss=1.1466 time=23.6s\n",
            "step=2,200 epoch=5/5 loss=1.0917\n",
            "step=2,300 epoch=5/5 loss=1.0542\n",
            "step=2,400 epoch=5/5 loss=1.0747\n",
            "step=2,500 epoch=5/5 loss=1.0814\n",
            "step=2,600 epoch=5/5 loss=1.0684\n",
            "step=2,700 epoch=5/5 loss=1.0545\n",
            "epoch=5/5 avg_loss=1.0756 time=23.9s\n",
            "Evaluating influence_aware on valid...\n",
            "{'num_eval_samples': 20000, 'hits@3': 0.20655, 'hits@5': 0.2962, 'hits@10': 0.41345, 'mrr': 0.14215620354413985}\n",
            "Evaluating influence_aware on test...\n",
            "{'num_eval_samples': 20000, 'hits@3': 0.20945, 'hits@5': 0.2993, 'hits@10': 0.41365, 'mrr': 0.14299196281433105}\n"
          ]
        }
      ],
      "source": [
        "# Load data and run experiments\n",
        "\n",
        "train_data, valid_data, test_data, entity_to_id, relation_to_id, time_to_id = load_data(DATA_DIR)\n",
        "true_tails = build_true_tails(train_data, valid_data, test_data)\n",
        "\n",
        "num_entities = len(entity_to_id)\n",
        "num_relations = len(relation_to_id)\n",
        "num_times = len(time_to_id)\n",
        "\n",
        "baseline_result = None\n",
        "influence_result = None\n",
        "\n",
        "if RUN_MODE in {\"baseline\", \"both\"}:\n",
        "    baseline_result = run_condition(\n",
        "        name=\"baseline\",\n",
        "        train_data=train_data,\n",
        "        valid_data=valid_data,\n",
        "        test_data=test_data,\n",
        "        true_tails=true_tails,\n",
        "        num_entities=num_entities,\n",
        "        num_relations=num_relations,\n",
        "        num_times=num_times,\n",
        "        influence_adjacency=None,\n",
        "        influence_lambda=0.0,\n",
        "    )\n",
        "\n",
        "if RUN_MODE in {\"influence\", \"both\"}:\n",
        "    influence_adjacency = load_influence_adjacency(\n",
        "        influence_graph_path=INFLUENCE_GRAPH_PATH,\n",
        "        entity_to_id=entity_to_id,\n",
        "        max_neighbors=MAX_INFLUENCE_NEIGHBORS,\n",
        "    )\n",
        "    influence_result = run_condition(\n",
        "        name=\"influence_aware\",\n",
        "        train_data=train_data,\n",
        "        valid_data=valid_data,\n",
        "        test_data=test_data,\n",
        "        true_tails=true_tails,\n",
        "        num_entities=num_entities,\n",
        "        num_relations=num_relations,\n",
        "        num_times=num_times,\n",
        "        influence_adjacency=influence_adjacency,\n",
        "        influence_lambda=INFLUENCE_LAMBDA,\n",
        "    )\n"
      ],
      "id": "HjMOg8C6DVrN"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skyJ1X9FDVrN",
        "outputId": "9c065d33-4315-4e08-b80f-68ef060babe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test delta (influence - baseline): {'hits@3': 0.0058, 'hits@5': 0.005850000000000022, 'hits@10': 0.0020000000000000018, 'mrr': 0.0017051832199096728}\n",
            "Saved: /content/RCS/TemporalKGs/phase3_results.json\n"
          ]
        }
      ],
      "source": [
        "# Save results\n",
        "\n",
        "results = {\n",
        "    \"config\": {\n",
        "        \"run_mode\": RUN_MODE,\n",
        "        \"emb_dim\": EMB_DIM,\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"eval_batch_size\": EVAL_BATCH_SIZE,\n",
        "        \"eval_max_samples\": EVAL_MAX_SAMPLES,\n",
        "        \"lr\": LR,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"influence_lambda\": INFLUENCE_LAMBDA,\n",
        "        \"max_influence_neighbors\": MAX_INFLUENCE_NEIGHBORS,\n",
        "        \"device\": str(device),\n",
        "    },\n",
        "    \"dataset_stats\": {\n",
        "        \"train\": len(train_data),\n",
        "        \"valid\": len(valid_data),\n",
        "        \"test\": len(test_data),\n",
        "        \"entities\": len(entity_to_id),\n",
        "        \"relations\": len(relation_to_id),\n",
        "        \"times\": len(time_to_id),\n",
        "    },\n",
        "    \"results\": {},\n",
        "}\n",
        "\n",
        "if baseline_result is not None:\n",
        "    results[\"results\"][\"baseline\"] = baseline_result\n",
        "if influence_result is not None:\n",
        "    results[\"results\"][\"influence_aware\"] = influence_result\n",
        "\n",
        "if baseline_result is not None and influence_result is not None:\n",
        "    delta_test = {}\n",
        "    for metric in (\"hits@3\", \"hits@5\", \"hits@10\", \"mrr\"):\n",
        "        delta_test[metric] = float(influence_result[\"test\"][metric]) - float(baseline_result[\"test\"][metric])\n",
        "    results[\"results\"][\"delta_test\"] = delta_test\n",
        "    print(\"\\nTest delta (influence - baseline):\", delta_test)\n",
        "\n",
        "out_path = OUTPUT_DIR / \"phase3_results.json\"\n",
        "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"Saved: {out_path}\")\n"
      ],
      "id": "skyJ1X9FDVrN"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}