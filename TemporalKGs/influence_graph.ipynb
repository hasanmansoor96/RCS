{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e23f1eb2",
        "outputId": "729e7128-6c44-4adf-e08c-6d62207ed463"
      },
      "source": [
        "import os\n",
        "\n",
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/hasanmansoor96/RCS\n",
        "\n",
        "# List the contents of the cloned repository to confirm\n",
        "repo_name = \"RCS\"\n",
        "if os.path.exists(repo_name):\n",
        "    print(f\"\\nContents of '{repo_name}':\")\n",
        "    !ls -F {repo_name}\n",
        "else:\n",
        "    print(f\"Error: Repository '{repo_name}' not found after cloning.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'RCS' already exists and is not an empty directory.\n",
            "\n",
            "Contents of 'RCS':\n",
            "TemporalKGs/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAIb5Tt-7Ega"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfaa7194",
        "outputId": "31bb9dda-83a3-45db-fcd0-69f03a593791"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Remove the existing RCS directory if it exists\n",
        "repo_name = \"RCS\"\n",
        "if os.path.exists(repo_name):\n",
        "    shutil.rmtree(repo_name)\n",
        "    print(f\"Removed existing directory '{repo_name}'.\")\n",
        "\n",
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/hasanmansoor96/RCS\n",
        "\n",
        "# List the contents of the cloned repository to confirm\n",
        "if os.path.exists(repo_name):\n",
        "    print(f\"\\nContents of '{repo_name}':\")\n",
        "    !ls -F {repo_name}\n",
        "else:\n",
        "    print(f\"Error: Repository '{repo_name}' not found after cloning.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed existing directory 'RCS'.\n",
            "Cloning into 'RCS'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 34 (delta 12), reused 16 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (34/34), 195.68 KiB | 6.31 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n",
            "Filtering content: 100% (5/5), 246.72 MiB | 42.34 MiB/s, done.\n",
            "\n",
            "Contents of 'RCS':\n",
            "TemporalKGs/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5bced9e",
        "outputId": "fc0de983-5da5-4ec9-e5a4-0563fdd8e3ba"
      },
      "source": [
        "import json\n",
        "\n",
        "json_dir = '/content/RCS/TemporalKGs/'\n",
        "\n",
        "granger_matrix = None\n",
        "embedding_similarity_matrix = None\n",
        "co_occurrence_matrix = None\n",
        "\n",
        "# Load granger_influence_matrix_optimized.json\n",
        "try:\n",
        "    with open(f'{json_dir}granger_influence_matrix_optimized.json', 'r') as f:\n",
        "        granger_matrix = json.load(f)\n",
        "    print(\"Loaded 'granger_influence_matrix_optimized.json' into 'granger_matrix'.\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error loading 'granger_influence_matrix_optimized.json': {e}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: 'granger_influence_matrix_optimized.json' not found at {json_dir}\")\n",
        "\n",
        "# Load emb_sim_matrix.json\n",
        "try:\n",
        "    with open(f'{json_dir}emb_sim_matrix.json', 'r') as f:\n",
        "        embedding_similarity_matrix = json.load(f)\n",
        "    print(\"Loaded 'emb_sim_matrix.json' into 'embedding_similarity_matrix'.\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error loading 'emb_sim_matrix.json': {e}\\nThis file appears to be corrupted or malformed JSON.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: 'emb_sim_matrix.json' not found at {json_dir}\")\n",
        "\n",
        "# Load sparse_influence_matrix.json\n",
        "try:\n",
        "    with open(f'{json_dir}sparse_influence_matrix.json', 'r') as f:\n",
        "        co_occurrence_matrix = json.load(f)\n",
        "    print(\"Loaded 'sparse_influence_matrix.json' into 'co_occurrence_matrix'.\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error loading 'sparse_influence_matrix.json': {e}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: 'sparse_influence_matrix.json' not found at {json_dir}\")\n",
        "\n",
        "# Check if all files were loaded successfully\n",
        "if granger_matrix is not None and embedding_similarity_matrix is not None and co_occurrence_matrix is not None:\n",
        "    print(\"All three JSON files loaded successfully.\")\n",
        "elif embedding_similarity_matrix is None:\n",
        "    print(\"Subtask partially completed: 'emb_sim_matrix.json' could not be loaded due to corruption. Cannot proceed with the full task without this file.\")\n",
        "else:\n",
        "    print(\"Subtask partially completed: Some files could not be loaded.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 'granger_influence_matrix_optimized.json' into 'granger_matrix'.\n",
            "Loaded 'emb_sim_matrix.json' into 'embedding_similarity_matrix'.\n",
            "Loaded 'sparse_influence_matrix.json' into 'co_occurrence_matrix'.\n",
            "All three JSON files loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4faaa16"
      },
      "source": [
        "# Task\n",
        "Generate a fused influence graph from the provided Granger causality matrix (`/content/RCS/TemporalKGs/granger_influence_matrix_optimized.json`), embedding similarity matrix (`/content/RCS/TemporalKGs/emb_sim_matrix.json`), and co-occurrence matrix (`/content/RCS/TemporalKGs/sparse_influence_matrix.json`). The process includes summarizing the embedding similarity matrix, building a master entity index, aligning and normalizing all three signals, fusing them using a weighted sum (co-occurrence: 0.4, Granger: 0.4, embedding similarity: 0.2), and then sparsifying the fused graph by retaining the top 30 neighbors and applying a minimum weight threshold of 0.05. Finally, export the resulting graph to `final_influence_graph.json` and provide a diagnostics report summarizing the process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "478e471b"
      },
      "source": [
        "## Summarize Embedding Similarity Matrix\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the loaded `embedding_similarity_matrix`, including the total number of entities and sample entries to confirm its structure and content. This confirms the successful loading and readiness of this crucial input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ed7d3c"
      },
      "source": [
        "**Reasoning**:\n",
        "To summarize the `embedding_similarity_matrix`, I will first print the total number of unique entities (keys) to understand its size, and then display a few sample entries to confirm its structure and content as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06f80959",
        "outputId": "24db10db-9d33-48bc-b2f1-60e6bdf96256"
      },
      "source": [
        "print(f\"Total number of unique entities in embedding_similarity_matrix: {len(embedding_similarity_matrix)}\")\n",
        "\n",
        "print(\"\\nSample entries from embedding_similarity_matrix:\")\n",
        "count = 0\n",
        "for entity, similarities in embedding_similarity_matrix.items():\n",
        "    if count < 3: # Print first 3 entries\n",
        "        print(f\"  Entity: '{entity}'\")\n",
        "        print(f\"    Sample Similarities: {list(similarities.items())[:3]}...\") # Print first 3 similarities for each entity\n",
        "        count += 1\n",
        "    else:\n",
        "        break"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique entities in embedding_similarity_matrix: 3413\n",
            "\n",
            "Sample entries from embedding_similarity_matrix:\n",
            "  Entity: 'Solomon Passy'\n",
            "    Sample Similarities: [('Women (Austria)', 0.0), ('Tyco International', 0.0), ('Piletji Sebola', 0.0)]...\n",
            "  Entity: 'Wolfgang SchÃ¼ssel'\n",
            "    Sample Similarities: [('Azali Assouman', 0.189811), ('Zapatista Army of National Liberation', 0.064076), ('Piletji Sebola', 0.081682)]...\n",
            "  Entity: 'Ho Mei-yueh'\n",
            "    Sample Similarities: [('Provisional Irish Republican Army', 0.0), ('Carlo Azeglio Ciampi', 0.0), ('Huang Ju', 0.203046)]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdb9b1c8"
      },
      "source": [
        "## Build Master Entity Index and Align Signals\n",
        "\n",
        "### Subtask:\n",
        "Extract all unique entities (nodes) from the loaded Granger, embedding similarity, and co-occurrence matrices to form a comprehensive master entity index. Create aligned data structures (e.g., pandas DataFrames or nested dictionaries) for each signal, ensuring all entities from the master index are present and missing values are handled (e.g., filled with 0).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3843f7d6"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin, I will create a `master_entity_set` and populate it with all unique entities from the keys of the `granger_matrix`, `embedding_similarity_matrix`, and `co_occurrence_matrix`. This ensures that every entity present in any of the three input matrices is captured.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02f3d148",
        "outputId": "59782fcd-8500-4e01-cc5d-eeff3b6f6e15"
      },
      "source": [
        "master_entity_set = set()\n",
        "\n",
        "# Extract entities from granger_matrix\n",
        "for entity in granger_matrix.keys():\n",
        "    master_entity_set.add(entity)\n",
        "    for target_entity in granger_matrix[entity].keys():\n",
        "        master_entity_set.add(target_entity)\n",
        "\n",
        "# Extract entities from embedding_similarity_matrix\n",
        "for entity in embedding_similarity_matrix.keys():\n",
        "    master_entity_set.add(entity)\n",
        "    for target_entity in embedding_similarity_matrix[entity].keys():\n",
        "        master_entity_set.add(target_entity)\n",
        "\n",
        "# Extract entities from co_occurrence_matrix\n",
        "for entity in co_occurrence_matrix.keys():\n",
        "    master_entity_set.add(entity)\n",
        "    for target_entity in co_occurrence_matrix[entity].keys():\n",
        "        master_entity_set.add(target_entity)\n",
        "\n",
        "print(f\"Master entity set created with {len(master_entity_set)} unique entities.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Master entity set created with 5396 unique entities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90ccf97e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `master_entity_set` is created, I will convert it into a sorted list named `master_entity_index`. Then, I will define the `align_matrix` helper function as described in the instructions to align each of the three matrices (`granger_matrix`, `embedding_similarity_matrix`, and `co_occurrence_matrix`) with the `master_entity_index`, handling missing values by setting them to 0. Finally, I will apply this function to create the aligned matrices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd17e866",
        "outputId": "0947d9dd-2334-4471-daad-16494bd6537f"
      },
      "source": [
        "master_entity_index = sorted(list(master_entity_set))\n",
        "\n",
        "print(f\"Master entity index created with {len(master_entity_index)} sorted entities.\")\n",
        "\n",
        "def align_matrix(original_matrix, master_index):\n",
        "    aligned_matrix = {}\n",
        "    for source_entity in master_index:\n",
        "        target_scores = {}\n",
        "        for target_entity in master_index:\n",
        "            # Get score, defaulting to 0 if not found\n",
        "            score = original_matrix.get(source_entity, {}).get(target_entity, 0)\n",
        "            target_scores[target_entity] = score\n",
        "        aligned_matrix[source_entity] = target_scores\n",
        "    return aligned_matrix\n",
        "\n",
        "# Align all three matrices\n",
        "aligned_granger_matrix = align_matrix(granger_matrix, master_entity_index)\n",
        "aligned_embedding_similarity_matrix = align_matrix(embedding_similarity_matrix, master_entity_index)\n",
        "aligned_co_occurrence_matrix = align_matrix(co_occurrence_matrix, master_entity_index)\n",
        "\n",
        "print(\"Granger matrix aligned successfully. Sample entry:\")\n",
        "# Print a sample entry from aligned_granger_matrix to verify\n",
        "if aligned_granger_matrix:\n",
        "    first_entity = next(iter(aligned_granger_matrix))\n",
        "    print(f\"  Entity: '{first_entity}'\")\n",
        "    print(f\"    Sample Target Scores: {list(aligned_granger_matrix[first_entity].items())[:3]}...\")\n",
        "\n",
        "print(\"Embedding similarity matrix aligned successfully. Sample entry:\")\n",
        "# Print a sample entry from aligned_embedding_similarity_matrix to verify\n",
        "if aligned_embedding_similarity_matrix:\n",
        "    first_entity = next(iter(aligned_embedding_similarity_matrix))\n",
        "    print(f\"  Entity: '{first_entity}'\")\n",
        "    print(f\"    Sample Target Scores: {list(aligned_embedding_similarity_matrix[first_entity].items())[:3]}...\")\n",
        "\n",
        "print(\"Co-occurrence matrix aligned successfully. Sample entry:\")\n",
        "# Print a sample entry from aligned_co_occurrence_matrix to verify\n",
        "if aligned_co_occurrence_matrix:\n",
        "    first_entity = next(iter(aligned_co_occurrence_matrix))\n",
        "    print(f\"  Entity: '{first_entity}'\")\n",
        "    print(f\"    Sample Target Scores: {list(aligned_co_occurrence_matrix[first_entity].items())[:3]}...\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Master entity index created with 5396 sorted entities.\n",
            "Granger matrix aligned successfully. Sample entry:\n",
            "  Entity: 'A.B. Bardhan'\n",
            "    Sample Target Scores: [('A.B. Bardhan', 0), ('A.K. Antony', 0), ('A.P. Sharma', 0)]...\n",
            "Embedding similarity matrix aligned successfully. Sample entry:\n",
            "  Entity: 'A.B. Bardhan'\n",
            "    Sample Target Scores: [('A.B. Bardhan', 0), ('A.K. Antony', 0), ('A.P. Sharma', 0)]...\n",
            "Co-occurrence matrix aligned successfully. Sample entry:\n",
            "  Entity: 'A.B. Bardhan'\n",
            "    Sample Target Scores: [('A.B. Bardhan', 0), ('A.K. Antony', 0), ('A.P. Sharma', 0)]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44806491"
      },
      "source": [
        "## Normalize Signals\n",
        "\n",
        "### Subtask:\n",
        "For each of the three aligned signals (Granger, Embedding Similarity, Co-occurrence), perform row-wise (per source entity) min-max normalization. Ensure robust handling of rows with zero variance (e.g., all zeros), typically by setting their normalized scores to 0 or 1 based on context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be63ad67"
      },
      "source": [
        "**Reasoning**:\n",
        "As instructed, I will define a function `normalize_matrix` to perform row-wise min-max normalization on the aligned matrices, handling cases of zero variance by setting normalized scores to 0. Then, I will apply this function to `aligned_granger_matrix`, `aligned_embedding_similarity_matrix`, and `aligned_co_occurrence_matrix`, and print sample entries to verify the normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51addb97",
        "outputId": "f2c69537-b416-4f3b-ec46-9ea75443ec59"
      },
      "source": [
        "def normalize_matrix(matrix, master_index):\n",
        "    normalized_matrix = {}\n",
        "    for source_entity in master_index:\n",
        "        target_scores = list(matrix.get(source_entity, {}).values())\n",
        "        if not target_scores: # Handle cases where source_entity might not be in the original matrix keys but is in master_index\n",
        "            normalized_matrix[source_entity] = {target_e: 0 for target_e in master_index}\n",
        "            continue\n",
        "\n",
        "        min_val = min(target_scores)\n",
        "        max_val = max(target_scores)\n",
        "\n",
        "        normalized_target_scores = {}\n",
        "        if min_val == max_val: # Handle zero variance case, set all to 0 as per instructions\n",
        "            for target_entity in master_index:\n",
        "                normalized_target_scores[target_entity] = 0.0\n",
        "        else:\n",
        "            for target_entity in master_index:\n",
        "                score = matrix.get(source_entity, {}).get(target_entity, 0.0) # Ensure score is float\n",
        "                normalized_score = (score - min_val) / (max_val - min_val)\n",
        "                normalized_target_scores[target_entity] = normalized_score\n",
        "        normalized_matrix[source_entity] = normalized_target_scores\n",
        "    return normalized_matrix\n",
        "\n",
        "# Apply normalization to all three aligned matrices\n",
        "normalized_granger_matrix = normalize_matrix(aligned_granger_matrix, master_entity_index)\n",
        "normalized_embedding_similarity_matrix = normalize_matrix(aligned_embedding_similarity_matrix, master_entity_index)\n",
        "normalized_co_occurrence_matrix = normalize_matrix(aligned_co_occurrence_matrix, master_entity_index)\n",
        "\n",
        "print(\"Granger matrix normalized successfully. Sample entry (original vs normalized):\")\n",
        "if normalized_granger_matrix:\n",
        "    first_entity = next(iter(master_entity_index))\n",
        "    print(f\"  Entity: '{first_entity}'\")\n",
        "    print(f\"    Original Sample Target Scores: {list(aligned_granger_matrix[first_entity].items())[:3]}...\")\n",
        "    print(f\"    Normalized Sample Target Scores: {list(normalized_granger_matrix[first_entity].items())[:3]}...\")\n",
        "\n",
        "print(\"\\nEmbedding similarity matrix normalized successfully. Sample entry (original vs normalized):\")\n",
        "if normalized_embedding_similarity_matrix:\n",
        "    first_entity = next(iter(master_entity_index))\n",
        "    print(f\"  Entity: '{first_entity}'\")\n",
        "    print(f\"    Original Sample Target Scores: {list(aligned_embedding_similarity_matrix[first_entity].items())[:3]}...\")\n",
        "    print(f\"    Normalized Sample Target Scores: {list(normalized_embedding_similarity_matrix[first_entity].items())[:3]}...\")\n",
        "\n",
        "print(\"\\nCo-occurrence matrix normalized successfully. Sample entry (original vs normalized):\")\n",
        "if normalized_co_occurrence_matrix:\n",
        "    first_entity = next(iter(master_entity_index))\n",
        "    print(f\"  Entity: '{first_entity}'\")\n",
        "    print(f\"    Original Sample Target Scores: {list(aligned_co_occurrence_matrix[first_entity].items())[:3]}...\")\n",
        "    print(f\"    Normalized Sample Target Scores: {list(normalized_co_occurrence_matrix[first_entity].items())[:3]}...\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Granger matrix normalized successfully. Sample entry (original vs normalized):\n",
            "  Entity: 'A.B. Bardhan'\n",
            "    Original Sample Target Scores: [('A.B. Bardhan', 0), ('A.K. Antony', 0), ('A.P. Sharma', 0)]...\n",
            "    Normalized Sample Target Scores: [('A.B. Bardhan', 0.0), ('A.K. Antony', 0.0), ('A.P. Sharma', 0.0)]...\n",
            "\n",
            "Embedding similarity matrix normalized successfully. Sample entry (original vs normalized):\n",
            "  Entity: 'A.B. Bardhan'\n",
            "    Original Sample Target Scores: [('A.B. Bardhan', 0), ('A.K. Antony', 0), ('A.P. Sharma', 0)]...\n",
            "    Normalized Sample Target Scores: [('A.B. Bardhan', 0.0), ('A.K. Antony', 0.0), ('A.P. Sharma', 0.0)]...\n",
            "\n",
            "Co-occurrence matrix normalized successfully. Sample entry (original vs normalized):\n",
            "  Entity: 'A.B. Bardhan'\n",
            "    Original Sample Target Scores: [('A.B. Bardhan', 0), ('A.K. Antony', 0), ('A.P. Sharma', 0)]...\n",
            "    Normalized Sample Target Scores: [('A.B. Bardhan', 0.0), ('A.K. Antony', 0.0), ('A.P. Sharma', 0.0)]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "940a626c"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to fuse the normalized matrices using the specified weighted sum. I will define a function to perform this weighted sum for each entity and its targets across the three normalized matrices and then apply it to create the `fused_influence_matrix`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea54b694",
        "outputId": "e2f7ac0a-8798-4963-86b4-ad740700e4f3"
      },
      "source": [
        "def fuse_matrices(granger_matrix, embedding_matrix, co_occurrence_matrix, master_index, weights):\n",
        "    fused_matrix = {}\n",
        "    for source_entity in master_index:\n",
        "        fused_target_scores = {}\n",
        "        for target_entity in master_index:\n",
        "            granger_score = granger_matrix.get(source_entity, {}).get(target_entity, 0.0)\n",
        "            embedding_score = embedding_matrix.get(source_entity, {}).get(target_entity, 0.0)\n",
        "            co_occurrence_score = co_occurrence_matrix.get(source_entity, {}).get(target_entity, 0.0)\n",
        "\n",
        "            fused_score = (\n",
        "                weights['co_occurrence'] * co_occurrence_score +\n",
        "                weights['granger'] * granger_score +\n",
        "                weights['embedding_similarity'] * embedding_score\n",
        "            )\n",
        "            fused_target_scores[target_entity] = fused_score\n",
        "        fused_matrix[source_entity] = fused_target_scores\n",
        "    return fused_matrix\n",
        "\n",
        "# Define weights\n",
        "weights = {\n",
        "    'co_occurrence': 0.4,\n",
        "    'granger': 0.4,\n",
        "    'embedding_similarity': 0.2\n",
        "}\n",
        "\n",
        "# Fuse the normalized matrices\n",
        "fused_influence_matrix = fuse_matrices(\n",
        "    normalized_granger_matrix,\n",
        "    normalized_embedding_similarity_matrix,\n",
        "    normalized_co_occurrence_matrix,\n",
        "    master_entity_index,\n",
        "    weights\n",
        ")\n",
        "\n",
        "print(\"Fused influence matrix created successfully. Sample entry:\")\n",
        "if fused_influence_matrix:\n",
        "    first_entity = next(iter(master_entity_index))\n",
        "    print(f\"  Entity: '{first_entity}'\")\n",
        "    print(f\"    Sample Fused Scores: {list(fused_influence_matrix[first_entity].items())[:5]}...\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused influence matrix created successfully. Sample entry:\n",
            "  Entity: 'A.B. Bardhan'\n",
            "    Sample Fused Scores: [('A.B. Bardhan', 0.0), ('A.K. Antony', 0.0), ('A.P. Sharma', 0.0), ('A.S.S. Hannan Shah', 0.0), ('ACT Party', 0.1803330732599747)]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "200fa760"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to sparsify the `fused_influence_matrix` as per the task instructions. This involves retaining only the top 30 neighbors for each source entity and applying a minimum weight threshold of 0.05. I will iterate through the fused matrix, filter based on the threshold, sort by score, and then select the top N neighbors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67b72690",
        "outputId": "7255670d-3797-4566-c167-e436fd5bbbb7"
      },
      "source": [
        "import json\n",
        "\n",
        "# Define sparsification parameters\n",
        "TOP_N_NEIGHBORS = 30\n",
        "MIN_WEIGHT_THRESHOLD = 0.05\n",
        "\n",
        "sparsified_influence_graph = {}\n",
        "for source_entity, target_scores in fused_influence_matrix.items():\n",
        "    # Filter targets by minimum weight threshold\n",
        "    filtered_targets = {target: score for target, score in target_scores.items() if score >= MIN_WEIGHT_THRESHOLD}\n",
        "\n",
        "    # Sort targets by score in descending order and get the top N\n",
        "    sorted_targets = sorted(filtered_targets.items(), key=lambda item: item[1], reverse=True)\n",
        "    top_n_targets = dict(sorted_targets[:TOP_N_NEIGHBORS])\n",
        "\n",
        "    if top_n_targets:\n",
        "        sparsified_influence_graph[source_entity] = top_n_targets\n",
        "\n",
        "print(f\"Sparsified influence graph created. Total entities with outgoing edges: {len(sparsified_influence_graph)}\")\n",
        "\n",
        "# Print a sample entry from the sparsified graph to verify\n",
        "print(\"\\nSample entry from sparsified_influence_graph:\")\n",
        "if sparsified_influence_graph:\n",
        "    first_entity = next(iter(sparsified_influence_graph))\n",
        "    print(f\"  Entity: '{first_entity}'\")\n",
        "    print(f\"    Sparsified Target Scores: {list(sparsified_influence_graph[first_entity].items())[:5]}...\")\n",
        "else:\n",
        "    print(\"  No entries in the sparsified graph. (This might indicate all scores were below threshold)\")\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsified influence graph created. Total entities with outgoing edges: 5395\n",
            "\n",
            "Sample entry from sparsified_influence_graph:\n",
            "  Entity: 'A.B. Bardhan'\n",
            "    Sparsified Target Scores: [('Islamic Extremist Group (Pakistan)', 0.8), ('Alliance for the Future', 0.7812499700547495), ('Aide Medicale Internationale', 0.4), ('Alvaro Leyva', 0.4), ('Criminal (Barbados)', 0.4)]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3fa01cf"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the influence graph has been sparsified, I will save the `sparsified_influence_graph` to a JSON file named `final_influence_graph.json` as specified in the task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "502c624f",
        "outputId": "a1241bfe-da2d-4bc0-c5d6-d7a48ea5e3e1"
      },
      "source": [
        "output_file_path = 'final_influence_graph.json'\n",
        "with open(output_file_path, 'w') as f:\n",
        "    json.dump(sparsified_influence_graph, f, indent=4)\n",
        "\n",
        "print(f\"Sparsified influence graph saved to '{output_file_path}'.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsified influence graph saved to 'final_influence_graph.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbde19b4"
      },
      "source": [
        "## Diagnostics Report\n",
        "\n",
        "This report summarizes the process of generating the fused influence graph:\n",
        "\n",
        "1.  **Data Loading**: Successfully loaded three JSON files:\n",
        "    *   `granger_influence_matrix_optimized.json` (into `granger_matrix`)\n",
        "    *   `emb_sim_matrix.json` (into `embedding_similarity_matrix`)\n",
        "    *   `sparse_influence_matrix.json` (into `co_occurrence_matrix`)\n",
        "\n",
        "2.  **Embedding Similarity Matrix Summary**: Confirmed `embedding_similarity_matrix` contained 3413 unique entities.\n",
        "\n",
        "3.  **Master Entity Index**: A `master_entity_index` was created, containing 5396 unique entities, combining all entities from the three input matrices.\n",
        "\n",
        "4.  **Signal Alignment**: All three matrices were successfully aligned to the `master_entity_index`, with missing values filled as 0.\n",
        "\n",
        "5.  **Signal Normalization**: Each aligned matrix (`granger`, `embedding similarity`, `co-occurrence`) was normalized row-wise (per source entity) using min-max scaling. Rows with zero variance had all their scores set to 0.\n",
        "\n",
        "6.  **Fusion**: The normalized matrices were fused into `fused_influence_matrix` using a weighted sum with the following weights:\n",
        "    *   Co-occurrence: 0.4\n",
        "    *   Granger: 0.4\n",
        "    *   Embedding Similarity: 0.2\n",
        "\n",
        "7.  **Sparsification**: The `fused_influence_matrix` was sparsified by:\n",
        "    *   Retaining only the top **30** neighbors for each source entity.\n",
        "    *   Applying a minimum weight threshold of **0.05**.\n",
        "    *   The resulting `sparsified_influence_graph` contains influence edges for **5395** entities.\n",
        "\n",
        "8.  **Output**: The final `sparsified_influence_graph` was successfully saved to `final_influence_graph.json`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f50f6c3"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial `embedding_similarity_matrix` contained 3413 unique entities.\n",
        "*   A comprehensive `master_entity_index` was created, encompassing 5396 unique entities from all three input matrices (Granger, embedding similarity, and co-occurrence).\n",
        "*   All three signals were successfully aligned to this master index, with missing values set to 0.\n",
        "*   Each aligned matrix underwent row-wise (per source entity) min-max normalization. Rows with zero variance had all their scores set to 0.\n",
        "*   The normalized matrices were fused using a weighted sum with the following coefficients: co-occurrence: 0.4, Granger: 0.4, and embedding similarity: 0.2.\n",
        "*   The fused influence matrix was sparsified by retaining the top 30 neighbors for each source entity and applying a minimum weight threshold of 0.05.\n",
        "*   The resulting `sparsified_influence_graph` contains influence edges for 5395 entities.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The successful fusion of diverse influence signals (temporal causality, semantic similarity, and co-occurrence) into a single graph provides a more comprehensive view of entity relationships than any single signal alone.\n",
        "*   Further analysis could involve evaluating the quality and utility of the fused graph for downstream tasks, such as link prediction or community detection, and experimenting with different weighting schemes or sparsification parameters to optimize performance.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}